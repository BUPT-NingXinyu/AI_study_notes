{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d49e7358-2234-49b9-850c-08f6c9b206c1",
   "metadata": {},
   "source": [
    "# 损失函数：多类支持向量机损失\n",
    "\n",
    "Multiclass SVM Loss\n",
    "\n",
    "$L_i = \\sum_{j\\ne y_i} max(0,s_j-s_{y_i}+1)\\,. $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ca7124-ef82-4f1c-99a6-eebf25a08573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def L_i_vectorized(x, y, W):\n",
    "    scores = W.dot(x)  # 计算得分\n",
    "    margins = np.maximum(0, scores - scores[y] + 1)  # 最大值计算\n",
    "    margins[y] = 0  # 正确类别对应的最大裕量 margins\n",
    "    loss_i = np.sum(margins)  # 求和运算\n",
    "    return loss_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd06596f-17bf-417c-8c2f-4e7ebd2c4361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算多类支持向量损失函数，非矢量化和半矢量化形式\n",
    "\n",
    "def L_i(x, y, W):\n",
    "    \"\"\"\n",
    "    未向量化版本.\n",
    "      对给定的单个样本(x,y)计算multiclass svm loss.\n",
    "      - x: 代表图片像素输入的向量 （例如CIFAR-10中是3073 x 1， 因为添加了bias项对应的1到x中）\n",
    "      - y: 图片对应的类别编号 （比如CIFAR-10 中是 0-9）\n",
    "      - W: 权重矩阵 （例如CIFAR-10 中是10 x 3073）\n",
    "    \"\"\"\n",
    "    delta = 1.0  # 设定delta\n",
    "    scores = W.dot(x)  # 内积计算得分\n",
    "    correct_class_score = scores[y]\n",
    "    D = W.shape[0]  # 类别数，例如10\n",
    "    loss_i = 0.0\n",
    "    for j in xrage(D):  # 遍历所有错误的类别\n",
    "        if j == y:\n",
    "            # 跳过正确类别\n",
    "            continue\n",
    "        # 对第 i 个样本累加 loss\n",
    "        loss_i += max(0, scores[j]-correct_class_score+delta)\n",
    "    return loss_i\n",
    "\n",
    "def L_i_vectorized(x, y, W):\n",
    "    \"\"\"\n",
    "    半向量化版本，速度更快。\n",
    "    之所以说是半向量化，是因为这个函数外层要用for循环遍历整个训练集\n",
    "    \"\"\"\n",
    "    delta = 1.0\n",
    "    scores = W.dot(x)\n",
    "    # 矩阵一次性计算\n",
    "    margins = np.maximum(0, scores-scores[y]+delta)\n",
    "    # 忽略正确类别\n",
    "    margins[y] = 0\n",
    "    loss_i = np.sum(margins)\n",
    "    return loss_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ebd7a1-68d3-43b0-a27d-4bf7e4274809",
   "metadata": {},
   "source": [
    "# 最优化： 策略 #1 ——随机搜索\n",
    "\n",
    "策略 #1：欠佳的解决思路：随机搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb509929-514a-4633-9fd4-7e5d15cdaea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 假设 X_train 是训练集 (例如. 3073 x 50, 000)\n",
    "# 假设 Y_train 是类别结果 （例如. 1D array of 50, 000)\n",
    "\n",
    "bestloss = float(\"inf\")  # 初始化一个最大的 float值\n",
    "for num in xrange(1000):\n",
    "    W = np.random.randn(10, 3073) * 0.0001  # 随机生成一组参数\n",
    "    loss = L(X_train, Y_train, W)  # 计算损失函数\n",
    "    if loss < bestloss:  # 比对已搜寻中最好的结果\n",
    "        bestloss = loss\n",
    "        bestW = W\n",
    "    print('in attempt $d the loss was %f, best %f' % (num, loss, bestloss))\n",
    "\n",
    "# 尝试在测试集上的性能\n",
    "# 假定 X_test 为 [3073 x 10000], Y_test 为 [10000 x 1]\n",
    "scores = Wbest.dot(Xte_cols)  # 10 x 10000, 计算类别得分\n",
    "# 找到最高得分作为结果\n",
    "Yte_predict = np.argmax(scores, axis = 0)\n",
    "# 计算准确度\n",
    "np.mean(Yte_predict == Yte)\n",
    "# 返回 0.1555"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8c7e07-adf6-4667-ada7-9aa03c1583bc",
   "metadata": {},
   "source": [
    "# 最优化：数值梯度计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15143a78-c6ed-4e1f-b55d-c84c591c0d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def eval_numerical_gradient(f, x):\n",
    "    \"\"\"\n",
    "    一个最基本的计算x点上f的梯度的算法\n",
    "    - f 为参数为x的函数\n",
    "    - x 是一个numpy的vector\n",
    "    \"\"\"\n",
    "    \n",
    "    fx = f(x)  # 计算原始点上的函数值\n",
    "    grad = np.zeros(x.shape)\n",
    "    h = 0.00001\n",
    "    \n",
    "    # 对x的每个维度都计算一遍\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        \n",
    "        # 计算 x+h 处的函数值\n",
    "        ix = it.multi_index\n",
    "        old_value = x[ix]\n",
    "        x[ix] = old_value + h  # 加 h\n",
    "        fxh = f(x)  # 计算 f(x + h)\n",
    "        x[ix] = old_value  # 存储之前的函数值\n",
    "        \n",
    "        # 计算偏导数\n",
    "        grad[ix] = (fxh - fx) / h  # 斜率\n",
    "        it.iternext()  # 开始下一个维度上的偏导计算\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699f7063-d14d-4ae9-ab0a-1197bdfaf791",
   "metadata": {},
   "source": [
    "# 最优化：梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ce73be-8bc5-4af4-8457-babf40fe3a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 伪代码\n",
    "while True:\n",
    "    weights_grad = evaluate_gradient(loss_fun, data, weights)\n",
    "    weights += - step_size * weights_grad  # 梯度下降更新参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a58a6f-accc-4a97-903d-021c9ea66cf1",
   "metadata": {},
   "source": [
    "# 最优化：小批量梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6cb642-c543-46aa-8882-0c5ace819b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 伪代码\n",
    "while True:\n",
    "    data_batch = sample_training_data(data, 256)  # 抽样256个样本作为一个batch\n",
    "    weights_grad = evaluate_gradient(loss_fun, data_batch, weights)\n",
    "    weights += - step_size * weights_grad  # 参数更新"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
