{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5fca6b5-218c-4a74-945a-cc795a12c4a5",
   "metadata": {},
   "source": [
    "# Vision Transformer(ViT)模型原理及PyTorch逐行实现\n",
    "\n",
    "来自b站up主deep_thoughts 合集【PyTorch源码教程与前沿人工智能算法复现讲解】\n",
    "\n",
    "P_28_Vision Transformer(ViT)模型原理及PyTorch逐行实现：\n",
    "\n",
    "https://www.bilibili.com/video/BV1cS4y1M7wo/?spm_id_from=pageDriver&vd_source=18e91d849da09d846f771c89a366ed40\n",
    "\n",
    "***论文***\n",
    "\n",
    "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale：\n",
    "\n",
    "https://arxiv.org/pdf/2010.11929.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d59e36-50f9-4ca1-ad67-a4686ccd7f3d",
   "metadata": {},
   "source": [
    " ## Transformer 模型特点\n",
    " \n",
    " * 无先验假设 （例如：局部关联性、有序建模性）\n",
    " * 核心计算在于自注意力机制，平方复杂度\n",
    " * 数据量的要求与归纳偏置的引入成反比\n",
    " \n",
    " 归纳偏置：人类通过归纳法所总结的经验，把经验带入到设计模型的过程之中"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505782de-93df-46f0-af1a-58c75a3be94c",
   "metadata": {},
   "source": [
    "## Transformer 使用类型\n",
    "\n",
    "* Encoder only： BERT、分类任务、非流式任务、**Vision Transformer（ViT）**\n",
    "* Decoder only： GPT系列、语言建模、自回归生成任务、流式任务\n",
    "* Encoder-Decoder： 机器翻译、语音识别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f8abf3-6979-4bef-90ce-2027b7b04919",
   "metadata": {},
   "source": [
    "## Vision Transformer（ViT）\n",
    "\n",
    "* DNN perspective\n",
    "  * image2patch\n",
    "  * patch2embedding\n",
    "* CNN perspective\n",
    "  * 2D convolution over image\n",
    "  * flatten the output feature map\n",
    "* class token embedding\n",
    "* position embedding\n",
    "  * interpolation when inference\n",
    "* Transformer Encoder\n",
    "* classification head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca5f494-fc75-45a9-a6b6-2529555a619c",
   "metadata": {},
   "source": [
    "## 代码实现\n",
    "\n",
    "***相关资料***\n",
    "\n",
    "torch.nn.Unfold 官方文档：https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html\n",
    "\n",
    "torch.tile 官方文档：https://pytorch.org/docs/stable/generated/torch.tile.html\n",
    "\n",
    "torch.nn.TransformerEncoder 官方文档：https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2467325-5c04-4ba9-a5e1-a9352aef9de5",
   "metadata": {},
   "source": [
    "## step1 convert image to embedding vector sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "283df65f-3795-4d34-90ed-3fc537c5b13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-5.3705,  5.9496, -3.7997,  1.7626, -6.0725,  6.5959,  3.7233,\n",
      "          -3.2709],\n",
      "         [ 9.6118,  4.9762, -0.4314,  4.9131,  1.5909,  1.2257,  8.3413,\n",
      "          -7.1560],\n",
      "         [10.8133, 11.2609,  3.9394,  0.3752, 10.3314,  3.6254, -6.2153,\n",
      "           0.1659],\n",
      "         [ 1.4727, -5.2586,  4.5698, -3.8365,  5.6321, -4.3481, -8.4019,\n",
      "           4.7010]]])\n",
      "tensor([[[-5.3705,  5.9496, -3.7997,  1.7626, -6.0725,  6.5959,  3.7233,\n",
      "          -3.2709],\n",
      "         [ 9.6118,  4.9762, -0.4314,  4.9131,  1.5909,  1.2257,  8.3413,\n",
      "          -7.1560],\n",
      "         [10.8133, 11.2609,  3.9394,  0.3752, 10.3314,  3.6254, -6.2153,\n",
      "           0.1659],\n",
      "         [ 1.4727, -5.2586,  4.5698, -3.8365,  5.6321, -4.3481, -8.4019,\n",
      "           4.7010]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def image2emb_naive(image, patch_size, weight):\n",
    "    # image shape: bs*channel*h*w\n",
    "    patch = F.unfold(image, kernel_size=patch_size, stride=patch_size).transpose(-1, -2)\n",
    "    patch_embedding = patch @ weight\n",
    "    return patch_embedding\n",
    "    \n",
    "def image2emb_conv(image, kernel, stride):\n",
    "    conv_output = F.conv2d(image, kernel, stride=stride)  # bs*oc*oh*ow\n",
    "    bs, oc, oh, ow = conv_output.shape\n",
    "    patch_embedding = conv_output.reshape((bs, oc, oh*ow)).transpose(-1, -2)\n",
    "    return patch_embedding\n",
    "\n",
    "# test code for image2emb\n",
    "bs, ic, image_h, image_w = 1, 3, 8, 8\n",
    "patch_size = 4\n",
    "model_dim = 8\n",
    "patch_depth = patch_size*patch_size*ic\n",
    "image = torch.randn(bs, ic, image_h, image_w)\n",
    "weight = torch.randn(patch_depth, model_dim)  # model_dim是输出通道数目，patch_depth是卷积核的面积乘以输入通道数\n",
    "\n",
    "patch_embedding_naive = image2emb_naive(image, patch_size, weight)  # 分块方法得到embedding\n",
    "kernel = weight.transpose(0, 1).reshape((-1, ic, patch_size, patch_size))  # oc*ic*kh*kw\n",
    "\n",
    "patch_embedding_conv = image2emb_conv(image, kernel, patch_size)  # 二维卷积方法得到embedding\n",
    "print(patch_embedding_naive)\n",
    "print(patch_embedding_conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b7f7c2-78ec-4af3-ad32-ab9c10698343",
   "metadata": {},
   "source": [
    "## step2 prepare CLS token embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c41c7cc6-63e9-4c59-a526-7952cd5e4696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "cls_token_embedding = torch.randn(bs, 1, model_dim, requires_grad=True)\n",
    "token_embedding = torch.cat([cls_token_embedding, patch_embedding_conv], dim=1)\n",
    "print(token_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d614547e-db62-47d8-9f69-37caabfb7431",
   "metadata": {},
   "source": [
    "## step3 add position embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b6e9306-637a-41d6-a7df-4b685f2233b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 8])\n",
      "torch.Size([1, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "max_num_token = 16\n",
    "\n",
    "position_embedding_table = torch.randn(max_num_token, model_dim, requires_grad=True)\n",
    "seq_len = token_embedding.shape[1]\n",
    "# ...\n",
    "# 忽略mask\n",
    "# ...\n",
    "print(position_embedding_table[:seq_len].shape)\n",
    "position_embedding = torch.tile(position_embedding_table[:seq_len], [token_embedding.shape[0], 1, 1])\n",
    "print(position_embedding.shape)\n",
    "\n",
    "token_embedding += position_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437840bd-76f7-4276-87c5-f29abe097f18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
