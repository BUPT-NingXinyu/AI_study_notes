{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48c52a57-d23e-494c-9af6-fb292d0658bb",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "![](./img/Transformer模型结构.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97160e42-d545-45c4-a8c0-07228c5ebe1f",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "* input word embedding：由稀疏的one-hot向量进入一个不带bias的FNN得到一个稠密的连续向量\n",
    "* position encoding\n",
    "  * 通过sin/cos来固定表征\n",
    "    * 每个位置确定性的\n",
    "    * 对于不同的句子，相同位置的距离一直\n",
    "    * 可以推广到更长的测试句子\n",
    "  * pe(pos+k)可以写成pe(pos)的线性组合\n",
    "  * 通过残差连接来使得位置信息流入深层\n",
    "* multi-head self-attention \n",
    "  * 使得建模能力更强，表征空间更丰富\n",
    "  * 由多组Q，K，V构成 每组单独计算一个attention向量\n",
    "  * 把每组的attention向量拼起来，并进入一个FFN得到最终的向量\n",
    "* feed-forward network\n",
    "  * 只考虑每个单独位置进行建模\n",
    "  * 不同位置参数共享\n",
    "  * 类似于1x1 pointwise convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818c734d-6100-4596-95b2-6d17e013fed6",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "* output word embedding\n",
    "* masked multi-head self-attention\n",
    "* multi-head cross-attention\n",
    "* feed-forward network\n",
    "* softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbd9dec-93de-47f6-9619-52b1954687ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
