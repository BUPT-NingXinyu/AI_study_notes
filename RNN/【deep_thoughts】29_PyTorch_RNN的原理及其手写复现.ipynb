{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69c53af9-0387-4dbb-99e1-437658f5fd82",
   "metadata": {},
   "source": [
    "# PyTorch RNN的原理及其手写复现\n",
    "\n",
    "来自b站up主deep_thoughts 合集【PyTorch源码教程与前沿人工智能算法复现讲解】\n",
    "\n",
    "P_29_PyTorch RNN的原理及其手写复现：\n",
    "\n",
    "https://www.bilibili.com/video/BV13i4y1R7jB/?spm_id_from=333.788&vd_source=18e91d849da09d846f771c89a366ed40\n",
    "\n",
    "RNN 官方文档： https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
    "\n",
    "torch.flip 官方文档： https://pytorch.org/docs/stable/generated/torch.flip.html\n",
    "\n",
    "***论文***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd34835-8d84-4aa9-82c2-69e4fd16c4e8",
   "metadata": {},
   "source": [
    "# 循环神经网络 RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd5c601-c9bf-4d60-9fda-81443064aba1",
   "metadata": {},
   "source": [
    "## 记忆单元分类\n",
    "* RNN\n",
    "* GRU\n",
    "* LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6080f5a1-93b5-4a27-9bce-2ecc53134121",
   "metadata": {},
   "source": [
    "## 模型类别\n",
    "* 单向循环\n",
    "* 双向循环\n",
    "* 多层单向或双向叠加"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe7e479-02aa-4cdf-b73e-40bbff10b526",
   "metadata": {},
   "source": [
    "## 优缺点\n",
    "* 优点\n",
    "  * 可处理变长序列\n",
    "  * 模型大小与序列长度无关\n",
    "  * 计算量与序列长度呈线性增长\n",
    "  * 考虑历史信息\n",
    "  * 便于流式输出\n",
    "  * 权重时不变\n",
    "* 缺点\n",
    "  * 串行计算比较慢\n",
    "  * 无法获取太长的历史信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cee1bd-9748-48f5-927d-3c0459fc2aa7",
   "metadata": {},
   "source": [
    "## 应用场景\n",
    "* AI诗歌生成\n",
    "* 文本情感分类\n",
    "* 词法识别\n",
    "* 机器翻译\n",
    "* 语音识别/生成\n",
    "* 语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0afeff70-952d-43e7-a484-fe666d00bbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e497fd7-0824-4fd1-9318-55e8d79bf535",
   "metadata": {},
   "source": [
    "## 1. 单向、单层RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7732d6e-4ecc-41ab-9f86-87e78ccda735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3])\n",
      "torch.Size([1, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "single_rnn = nn.RNN(4, 3, 1, batch_first=True)\n",
    "input = torch.randn(1, 2, 4)  # bs * sl * fs\n",
    "output, h_n = single_rnn(input)\n",
    "print(output.shape)\n",
    "print(h_n.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f0816b-5b6b-4c21-91d0-00434bff546d",
   "metadata": {},
   "source": [
    "## 2. 双向、单层RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fae1b3ce-9e7a-4527-b00a-7136af90a500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 6])\n",
      "torch.Size([2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "bidirectional_rnn = nn.RNN(4, 3, 1, batch_first=True, bidirectional=True)\n",
    "bi_output, bi_h_n = bidirectional_rnn(input)\n",
    "print(bi_output.shape)\n",
    "print(bi_h_n.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98986bd9-da53-4594-98ff-0084cccb94aa",
   "metadata": {},
   "source": [
    "## 单向RNN与双向RNN的逐行实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ff12e42-d3d3-4991-b01f-c60539fae89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch API output:\n",
      "tensor([[[-0.0021,  0.1526,  0.6347],\n",
      "         [-0.5690,  0.1357,  0.4924],\n",
      "         [ 0.5337, -0.0540,  0.9681]],\n",
      "\n",
      "        [[ 0.3480, -0.0049,  0.8693],\n",
      "         [-0.0484, -0.1439,  0.9146],\n",
      "         [ 0.6201,  0.0670,  0.9676]]], grad_fn=<TransposeBackward1>)\n",
      "tensor([[[ 0.5337, -0.0540,  0.9681],\n",
      "         [ 0.6201,  0.0670,  0.9676]]], grad_fn=<StackBackward0>)\n",
      "\n",
      " custom rnn_forward function output:\n",
      "tensor([[[-0.0021,  0.1526,  0.6347],\n",
      "         [-0.5690,  0.1357,  0.4924],\n",
      "         [ 0.5337, -0.0540,  0.9681]],\n",
      "\n",
      "        [[ 0.3480, -0.0049,  0.8693],\n",
      "         [-0.0484, -0.1439,  0.9146],\n",
      "         [ 0.6201,  0.0670,  0.9676]]], grad_fn=<CopySlices>)\n",
      "tensor([[[ 0.5337, -0.0540,  0.9681],\n",
      "         [ 0.6201,  0.0670,  0.9676]]], grad_fn=<UnsqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "bs, T = 2, 3  # 批大小，输入序列长度\n",
    "input_size, hidden_size = 2, 3  # 输入特征大小，隐含层特征大小\n",
    "input = torch.randn(bs, T, input_size)  # 随机初始化一个输入特征序列\n",
    "h_prev = torch.zeros(bs, hidden_size)  # 初始隐含状态\n",
    "\n",
    "# step1 调用PyTorch RNN API\n",
    "rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "rnn_output, state_final = rnn(input, h_prev.unsqueeze(0))\n",
    "print(\"PyTorch API output:\")\n",
    "print(rnn_output)\n",
    "print(state_final)\n",
    "\n",
    "# step2 手写一个rnn_forward函数，实现单向RNN的计算原理\n",
    "def rnn_forward(input, weight_ih, weight_hh, bias_ih, bias_hh, h_prev):\n",
    "    bs, T, input_size = input.shape\n",
    "    h_dim = weight_ih.shape[0]\n",
    "    h_out = torch.zeros(bs, T, h_dim)  # 初始化一个输出（状态）矩阵\n",
    "    \n",
    "    for t in range(T):\n",
    "        x = input[:, t, :].unsqueeze(2)  # 获取当前时刻输入特征, bs * input_size\n",
    "        w_ih_batch = weight_ih.unsqueeze(0).tile(bs, 1, 1)  # bs*h_dim+input_size\n",
    "        w_hh_batch = weight_hh.unsqueeze(0).tile(bs, 1, 1)  # bs*h_dim*h_dim\n",
    "        \n",
    "        w_times_x = torch.bmm(w_ih_batch, x).squeeze(-1)  # bs*h_dim\n",
    "        w_times_h = torch.bmm(w_hh_batch, h_prev.unsqueeze(2)).squeeze(-1)  # bs*h_dim\n",
    "        h_prev = torch.tanh(w_times_x+bias_ih+w_times_h+bias_hh)\n",
    "        \n",
    "        h_out[:, t, :] = h_prev\n",
    "        \n",
    "    return h_out, h_prev.unsqueeze(0)\n",
    "\n",
    "# 验证一下rnn_forward的正确性\n",
    "#for k,v in rnn.named_parameters():\n",
    "    #print(k, v)\n",
    "custom_rnn_output, custom_state_final = rnn_forward(input, rnn.weight_ih_l0, rnn.weight_hh_l0, rnn.bias_ih_l0, rnn.bias_hh_l0, h_prev)\n",
    "\n",
    "print(\"\\n custom rnn_forward function output:\")\n",
    "print(custom_rnn_output)\n",
    "print(custom_state_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f63a7fd-7b0e-4919-8efc-0fb5f2458e89",
   "metadata": {},
   "source": [
    "## 手写双向RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "447dd5b9-14c9-4ff0-b534-cee7c8a57248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch API output:\n",
      "tensor([[[ 0.4750,  0.4664,  0.1212, -0.5255, -0.5600,  0.3535],\n",
      "         [-0.0551,  0.7235,  0.1034, -0.2400, -0.9647, -0.3409],\n",
      "         [ 0.4662, -0.3591, -0.8366,  0.0743, -0.2414,  0.8352]],\n",
      "\n",
      "        [[ 0.6957, -0.0872, -0.4927, -0.3050, -0.3211,  0.3237],\n",
      "         [ 0.6182, -0.3142, -0.6594,  0.2009, -0.6185,  0.6459],\n",
      "         [ 0.7947, -0.5041, -0.6501,  0.0895,  0.2259,  0.8508]]],\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[ 0.4662, -0.3591, -0.8366],\n",
      "         [ 0.7947, -0.5041, -0.6501]],\n",
      "\n",
      "        [[-0.5255, -0.5600,  0.3535],\n",
      "         [-0.3050, -0.3211,  0.3237]]], grad_fn=<StackBackward0>)\n",
      "\n",
      " custom bidirectional_rnn_forward function output:\n",
      "tensor([[[ 0.4750,  0.4664,  0.1212,  0.0743, -0.2414,  0.8352],\n",
      "         [-0.0551,  0.7235,  0.1034, -0.2400, -0.9647, -0.3409],\n",
      "         [ 0.4662, -0.3591, -0.8366, -0.5255, -0.5600,  0.3535]],\n",
      "\n",
      "        [[ 0.6957, -0.0872, -0.4927,  0.0895,  0.2259,  0.8508],\n",
      "         [ 0.6182, -0.3142, -0.6594,  0.2009, -0.6185,  0.6459],\n",
      "         [ 0.7947, -0.5041, -0.6501, -0.3050, -0.3211,  0.3237]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[[ 0.4662, -0.3591, -0.8366],\n",
      "         [ 0.7947, -0.5041, -0.6501]],\n",
      "\n",
      "        [[-0.5255, -0.5600,  0.3535],\n",
      "         [-0.3050, -0.3211,  0.3237]]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# step3 手写一个bidirectional_rnn_forward函数，实现双向RNN的计算原理\n",
    "def bidirectional_rnn_forward(input, weight_ih, weight_hh, bias_ih, bias_hh, h_prev, \\\n",
    "                             weight_ih_reverse, weight_hh_reverse, bias_ih_reverse, bias_hh_reverse, h_prev_reverse):\n",
    "    bs, T, input_size = input.shape\n",
    "    h_dim = weight_ih.shape[0]\n",
    "    h_out = torch.zeros(bs, T, h_dim*2)  # 初始化一个输出（状态）矩阵，注意双向是两倍的特征大小\n",
    "    \n",
    "    forward_output = rnn_forward(input, weight_ih, weight_hh, bias_ih, bias_hh, h_prev)[0]  # forward layer\n",
    "    backward_output = rnn_forward(torch.flip(input, [1]), weight_ih_reverse, weight_hh_reverse, bias_ih_reverse, bias_hh_reverse, h_prev_reverse)[0]  # backward layer\n",
    "    \n",
    "    h_out[:, :, :h_dim] = forward_output\n",
    "    h_out[:, :, h_dim:] = backward_output\n",
    "    \n",
    "    return h_out, h_out[:, -1, :].reshape((bs, 2, h_dim)).transpose(0, 1)\n",
    "\n",
    "# 验证一下bidirectional_rnn_forward的正确性\n",
    "bi_rnn = nn.RNN(input_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "h_prev = torch.zeros(2, bs, hidden_size)\n",
    "bi_rnn_output, bi_state_final = bi_rnn(input, h_prev)\n",
    "\n",
    "#for k,v in bi_rnn.named_parameters():\n",
    "    #print(k, v)\n",
    "\n",
    "custom_bi_rnn_output, custom_bi_state_final = bidirectional_rnn_forward(input, bi_rnn.weight_ih_l0, bi_rnn.weight_hh_l0, bi_rnn.bias_ih_l0, bi_rnn.bias_hh_l0, h_prev[0], \\\n",
    "                             bi_rnn.weight_ih_l0_reverse, bi_rnn.weight_hh_l0_reverse, bi_rnn.bias_ih_l0_reverse, bi_rnn.bias_hh_l0_reverse, h_prev[1])\n",
    "\n",
    "print(\"PyTorch API output:\")\n",
    "print(bi_rnn_output)\n",
    "print(bi_state_final)\n",
    "\n",
    "print(\"\\n custom bidirectional_rnn_forward function output:\")\n",
    "print(custom_bi_rnn_output)\n",
    "print(custom_bi_state_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37d4dbe-8f00-4980-99ed-a02b6c411836",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
